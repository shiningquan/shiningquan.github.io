[{"title":"默读","date":"2021-03-25T08:51:53.000Z","url":"/2021/03/25/%E9%BB%98%E8%AF%BB/","tags":[["耽美","/tags/%E8%80%BD%E7%BE%8E/"]],"categories":[["小说笔记","/categories/%E5%B0%8F%E8%AF%B4%E7%AC%94%E8%AE%B0/"]]},{"title":"Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference","date":"2020-11-03T11:19:57.000Z","url":"/2020/11/03/Stolen-Memories-Leveraging-Model-Memorization-for-Calibrated-White-Box-Membership-Inference/","tags":[["Membership Inference","/tags/Membership-Inference/"],["USNIX","/tags/USNIX/"]],"categories":[["论文笔记","/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"]],"content":"基本信息论文来源Klas Leino and Matt Fredrikson. ``Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference’’ Proceedings of the 29th USENIX Security Symposium, 2020. 概述本文针对白盒设置下的成员推理攻击进行了讨论，首先在线性模型中运用贝叶斯优化进行攻击，进而将攻击思路推广到深度模型中，并提出对攻击的精度进行校准。最后本文对攻击方法进行了防御评估。 论文要点背景机器学习的许多应用涉及敏感个人数据的收集和处理，这引起了我们对于隐私的担忧。特别是，将机器学习算法应用于私人训练数据时，生成的模型可能会通过其行为或表示，无意间泄露有关该数据的信息。成员推理攻击旨在确定在用于构建模型的训练集中是否存在给定的数据点。迄今为止，大多数成员推理攻击都遵循影子模型方法，Nasr等人将该方法扩展到白盒设置，但是他们发现将影子模型方法简单扩展到白盒设置不会产生有效的攻击。因此，本文基于这个背景，研究并提出一种有效的白盒成员推理攻击。 问题陈述目前的成员推理攻击方法大多是在黑盒设置下，虽然Nasr等人从目标模型获得激活和梯度信息，并将其作为攻击模型的功能，以此将攻击扩展到白盒设置中。但是他们发现这种简单扩展不会产生有效的攻击，因此他们假设对手已经知道目标模型的训练数据的很大一部分，但这种白盒攻击偏离了大多数针对成员推理的工作所共有的威胁模型。 因此本文重新审视了白盒成员推理的问题，提出了一种有效的白盒成员推理攻击，这种攻击无需访问目标模型的任何训练数据即可进行操作。并且，本文的分析揭示了对模型中过度拟合如何发生的更深入的了解，即训练数据中存在的特有特征（仅对训练数据具有预测性，而对采样分布没有预测性）通常在训练过程中被编码在模型中。 威胁模型 本文展示了如何明确识别深度网络中的记忆，并利用它来进行成员推理。成员信息通过目标模型的特有功能泄漏，而训练数据中分布的特征与普通人群中分布的特征不同，这些特征提供了支持或反对成员资格的证据。 本文发现可以对攻击进行校准，以增强对肯定推理的信心。 本文评估了针对本文攻击方法的常用防御的实用性。 主要贡献 本文展示了如何明确识别深度网络中的记忆，并利用它来进行成员推理。成员信息通过目标模型的特有功能泄漏，而训练数据中分布的特征与普通人群中分布的特征不同，这些特征提供了支持或反对成员资格的证据。 本文发现可以对攻击进行校准，以增强对肯定推理的信心。 本文评估了针对本文攻击方法的常用防御的实用性。 方案 图1展示了两个高斯分布$\\eta^{}$和$\\hat{\\eta}$的示例，其中$\\eta^{}$表示普通人群、$\\hat{\\eta}$表示训练集。如果$x^{\\prime}$满足公式1，则我们认为$x^{\\prime}$更可能从$\\hat{\\eta}$中提取，而不是$\\eta^{*}$。因此我们可以创建一个分类器来判断$x^{\\prime}$是从哪个分布里提取的。 \\begin{equation} Pr_{\\eta^{*}}\\left[x^{\\prime}\\right]&lt;Pr_{\\hat{\\eta}}\\left[x^{\\prime}\\right] \\end{equation} 贝叶斯最优成员推理bayes-wb攻击假设： 假设数据符合高斯分布，并且满足朴素贝叶斯假设 使用经验均值和协方差将训练集建模为高斯分布 假设目标模型是线性模型 由上述思想，攻击者可以利用朴素贝叶斯假设进行成员推理攻击，也就是说，攻击者可以将给定标记的$x$的观测概率写作独立观测$x$的每个特征的概率的乘积。公式2给出成员资格的贝叶斯最优预测器。其中攻击模型$m^{y}(x)$被定义为，给出点$(x,y)$属于训练集$S$的概率。 \\begin{equation}m^{y}(x)=\\mathcal{\\delta}\\left(w^{y T} x+b^{y}\\right)\\end{equation} \\begin{center} where $\\qquad w^{y}=\\frac{\\hat{\\mu}{y}-\\mu{y}^{}}{\\sigma^{2}} \\qquad b^{y}=\\sum_{j} \\frac{\\mu_{y j}^{ 2}-\\hat{\\mu}{y j}^{2}}{2 \\sigma{j}^{2}}$\\end{center} 在现实中，攻击者并不能准确的知道分布$\\hat{\\mathcal{D}}$和$\\mathcal{D}^{}$的参数，但是目标模型$\\hat{g}$学习到的权重会对$\\hat{\\mathcal{D}}$更加敏感，因此攻击者可以使用权重来对有关$\\hat{\\mathcal{D}}$和$\\mathcal{D}^{}$的差异的有用信息进行编码。具体攻击流程如下： 在辅助数据上训练代理模型。 将代理模型的权重与目标模型的权重进行比较，以创建攻击模型。 将样本点$(x,y)$输入攻击模型，模型会给出判断结果。因为在目标模型中使用的特征与在代理模型中使用的特征不同，因此它可以用于确定成员资格。 任意分布成员推理general-wb bayes-wb攻击通过测量目标模型的权重与代理模型所近似的真实分布的理想权重之间的某种位移来加权成员预测。因此本文定义了一个位移函数$d_f$，将其逐元素应用于模型的权重，这样可以将bayes-wb应用到任意分布中。在高斯朴素贝叶斯假设下，逐元素减法（即$d_f (x,y)=x-y$）是进行成员推理的最佳选择，但对于其他分布而言，不同的位移函数可能更合适。 图2显示了general-wb的模型，将学习到的位移函数$d_f$逐元素应用于目标和代理模型的权重，以生成攻击模型权重$W$，同时也应用于目标和代理模型的偏置，以生成攻击模型的偏置$b$。然后，使用$W$和$x$的内积进行成员预测。 深度模型中的成员推理我们可以将上述思想应用于深度网络的各个层中，但是在这之前我们需要将网络分解为两个函数，即$f=g \\circ h$。其中，$h$计算特征，$g$使用这些特征进行分类。 对于网络顶层，$g$是一个线性模型，可以直接应用上述算法。对于网络中低层，$g$不再是线性的，但是我们可以通过局部线性近似来将$g$近似为线性的，然后应用上述算法，如图3。 由于模型的内部表示在各层之间并不是独立的，因此我们不能简单地将每一层的近似权重串联起来，并将其视为对单个模型的攻击。相对的，我们可以使用一个元模型，该模型学习如何组合各个分层攻击的逻辑输出。 评估评价指标攻击的准确度是指$\\mathcal{A}$的预测等于$b$的概率。由于攻击者随机猜测可以达到50％的准确度，因此我们通常选择描述攻击的优势，即公式2给出的。优势将准确度缩放到基线的50％，以得出介于-1和1的值。 \\begin{equation}advantage(\\mathcal{A})=2Pr[\\mathcal{A}((x, y), aux(\\hat{g}))=b]-1\\end{equation} 如果对手可以自信地识别出任何数据点，就会发生侵犯隐私的行为，这样造成的威胁要比攻击者以较低的信心识别每个训练人员所造成的威胁大得多。因此，我们还将精度（等式3）视为攻击者的关键目标。为了使攻击者能够得出可靠的推论，精度必须明显大于1/2。如果没有点被预测为成员，则将精度定义为1/2。 \\begin{equation}precision(\\mathcal{A})=Pr[b=1 \\mid \\mathcal{A}((x, y), aux(\\hat{g}))=1]\\end{equation} 最后，召回率（等式4）也是一项评估指标。但是，我们对这个指标的重视程度较低，因为如果无法在任何点上返回可信的推断，则具有较高召回率的攻击在实践中不一定有效。 \\begin{equation}recall(\\mathcal{A})=Pr[\\mathcal{A}((x, y), aux(\\hat{g}))=1 \\mid b=1]\\end{equation} 数据集本文针对合成数据和源自真实数据的九个分类数据集进行了实验。通常，从医学和金融等领域选择数据集，本文为了便于与以前的工作进行比较，还包括了三个常见的图像数据集（MNIST，CIFAR10和CIFAR100）。 合成数据集具有10个类别，75个要素，数据记录量分别为400、800或1600，每类具有相等数量的记录。合成数据的特征是从多元高斯分布中随机抽取的。 分类数据集包含，成年人，Pima糖尿病；威斯康星州乳腺癌，肝炎，德国信贷，LFW；MNIST，CIFAR10和CIFAR100。图4显示了每个数据集的特征。 评估结果在评估每种攻击时，本文将数据随机分为三个不相交的组：训练，测试和保持。训练组和测试组各占实例总数的四分之一，而保留组则占实例总数的剩余一半。目标模型在训练组上进行了训练，而攻击者只允许使用保留组，在训练组（成员）和测试组（非成员）上评估了攻击模型的预测，每个实验在数据拆分的不同随机采样上重复10次，并取平均值。 在整个评估过程中，我们评估了四种不同的攻击：天真，bayes-wb，general-wb和shadow-bb。天真的攻击是指，当且仅当x正确分类时，攻击模型才能预测实例x是训练集的成员。shadow-bb是指黑盒阴影模型攻击。 我们可以将无所不知的攻击视为对高斯朴素贝叶斯数据进行白盒攻击的预期准确性的上限，因为这是真正的贝叶斯最优攻击。本文的攻击平均获得了全知攻击的84％的优势，这表明代理模型能够根据需要大致捕获总体分布，以检测目标模型对特征的特质使用。 当代表位移函数的神经网络被赋予足够的能力来重现bayes-wb攻击时，general-wb平均恢复了bayes-wb攻击优势的94％。通过检查位移网络的权重，我们发现general-wb几乎精确地学习了逐元素减法，这表明了其学习最佳位移函数的潜力。如果容量过大，则general-wb攻击的性能仅会稍差一些，平均可达到最低general-wb攻击优势的92％（Bayes-wb的86％），这表明General-wb不太容易过度拟合。 当然，我们发现这种模式对于现实世界的数据集也同样有效。在攻击真实数据集Adult时，我们观察到，随着更多数据可用于训练，攻击的优势逐渐减弱，但这个优势在整个数据集上变化得很小（\\textless4％）。这可能表明Adult数据集足够大，可以通过标准训练获得的中等大小的MLP模型来防止任何重大的信息泄漏。 评论扩展阅读Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Stand-alone and federated learning under passive and active white-box inference attacks. CoRR, abs/1812.00910, 2018. Reza Shokri, Marco Stronati, and Vitaly Shmatikov. Membership inference attacks against machine learning models. CoRR, abs/1610.05820, 2016. 启示相对于先前的工作，这篇论文改进的地方是用到了模型内部的信息，可以让攻击者对自己推理结果更加有信心，但他仍然有影子模型的思想在里面。本文中提到每个类别可能具有不同的均值，因此必须使用单独的标准来将它们区分开，但是作者直接假定为对手提供了真实的类别标签，所以没有在这个地方多做说明，而我认为我可以在我的工作中利用这一点，因为我想要推理出来的是一个类。"},{"title":"Comprehensive Privacy Analysis of Deep Learning","date":"2019-10-09T10:58:34.000Z","url":"/2019/10/09/Comprehensive-Privacy-Analysis-of-Deep-Learning/","tags":[["S&P","/tags/S-P/"],["Membership Inference","/tags/Membership-Inference/"]],"categories":[["论文笔记","/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"]],"content":"论文信息标题：Comprehensive Privacy Analysis of Deep Learning 作者：Milad Nasr, Reza Shokri, Amir Houmansadr 出处：S&amp;P 年份：2019 概述本文对深度学习模型上的白盒隐私推理攻击进行了详细的分析。本文针对被动和主动推理攻击这，在独立和联合设置下设计攻击，并假设对手具有不同的先验知识，证明了即使是通用化的模型也极易受到白盒成员推理攻击的影响。 背景应用场景许多应用程序和服务在大范围（且可能敏感）的用户数据上使用深度学习算法，其中包括用户语音，图像以及医疗，财务，社交和位置数据点。 问题陈述提出问题：深度学习算法对于单个训练数据样本的信息泄露量是多少？ 定义对于目标训练数据记录的一个模型的敏感隐私泄露为：攻击者可以推理出的关于数据记录的信息，并且不能由未使用该训练数据的相似模型推理得出。 价值论文贡献本文提供了一个使用白盒成员推理攻击进行深度神经网络隐私分析的综合框架，并且从以下方面对该框架进行了评估：独立/联合设置；攻击完美训练/进行了微调的模型；无监督攻击；被动/主动推理攻击。 证明了即使是通用化的模型也极易受到白盒成员推理攻击的影响；即使使用预先训练的最先进的目标模型，攻击仍然有效。 论文弱点本文只针对论文中提出的攻击方法进行了详细描述和评估，通过实验结果可以看到攻击的准确率几乎都在70%及以上。本文在文章最后提到了差分隐私，说差分隐私是一个强大的防御方法，但是又说本文的攻击者可以通过观察参数来推理大量的私人信息，并不清楚差分隐私能不能防御本文的攻击方法。 方案技术依据深度神经网络会记住有关其训练数据的信息，因此容易受到推理攻击。并且在白盒背景下，攻击者可以获取模型（包括模型参数），进而可以计算每一层的输出，梯度还有损失。 本文利用随机梯度下降（SGD）算法的隐私漏洞来设计白盒推理攻击。因为为了最大程度的减少模型的预期损失，SGD算法会在整个训练数据集的损失梯度趋于零的方向上反复更新模型参数。因此，每个训练数据样本都会在模型参数上的损失函数的局部梯度上留下明显的足迹。 攻击者利用这些可以获取的信息进行攻击，并且评估在各种场景下的训练数据泄露程度。 方案框架给定目标数据(x,y)，攻击者在目标输入x上运行目标模型f，计算所有隐藏层、模型输出和损失函数。攻击者还会在反向传播时计算相对于每个层的参数的损耗梯度。这些计算构成了推理攻击的输入特征。 攻击模型由卷积神经网络(CNN)和全连接网络组成(FCN)，攻击者对每个攻击特征观察多次，并在将它们传递到相应的攻击组件之前对其进行堆叠。CNN和FCN组件的输出附加到一起，并且此向量传递到完全连接的编码器，利用编码器和多个隐藏层来组合所有攻击特征提取组件的输出。编码器的输出是单个分数，即攻击的输出，预测了输入数据的隶属概率。 评估评价标准Attack accuracy：对于一个未知的数据点的正确的成员资格预测的分数 True/False positive：True positive代表本来是成员，分类成成员；False positive代表本来不是成员，分类成成员 Prediction uncertainty：使用对于给定输入的预测向量的归一化熵来计算 评估结果场景1：独立学习；攻击完美训练的模型；CIFAR100 证明了模型最后一层的输出泄露的成员信息最多；相对于模型输出来说，梯度会泄露更多的成员信息；相较于模型的其它层，最后一层的梯度泄露更多的成员信息；训练集大小、成员和非成员的比例将影响攻击准确性；成员的梯度范式随着训练周期下降；拥有高预测不准确性的类会泄露更多成员信息。 场景2：独立学习；无监督攻击；CIFAR100，Texas100，Purchase100 从实验结果来看，本文的攻击在当前场景下提供了很高的攻击准确度 场景3：独立学习；攻击微调（为了更好的训练准确度）模型 攻击者可以区分成员（分别在训练模型的数据集里和训练微调模型的数据集里）和非成员，还可以区分两个数据集的成员。 场景4：联邦学习；被动推理攻击（默默观察） 被动全局攻击者（参数聚合器）。对于CIFAR100数据集，达到了一个较高的准确度，但是对于Texas100和Purchase100，与独立学习相比准确度下降。 被动本地攻击者（学习参与者）。相较于全局攻击，准确度低，因为只能观察聚合之后的参数更新，限制了信息泄露。 场景5：联邦学习；主动推理攻击（会攻击性的修改参数更新） 梯度上升攻击：相较被动全局攻击，主动全局攻击获得更高的准确度。本地攻击的准确度低于全局攻击。 孤立攻击：攻击者隔离目标参与者和其学习进程。隔离之后目标参与者的模型不会进行聚合，并且存储了很多信息。 仅仅运用孤立，攻击准确度上升。将梯度上升和孤立合起来运用，攻击准确度进一步提高。"},{"title":"越南五日行","date":"2019-02-05T12:45:17.000Z","url":"/2019/02/05/%E8%B6%8A%E5%8D%97%E4%BA%94%E6%97%A5%E8%A1%8C/","tags":[["越南","/tags/%E8%B6%8A%E5%8D%97/"],["游记","/tags/%E6%B8%B8%E8%AE%B0/"]],"categories":[["游记","/categories/%E6%B8%B8%E8%AE%B0/"]],"content":"写在前面的话： 千万千万别坐红眼航班，呜呜呜太难受了，好好的睡觉时间要在飞机上度过*(:з」∠)* 在办落地签的那个地方（也就是下飞机后过关以前）买电话卡的话，七天无限流量20万越南盾，十天无限流量本地通话100分钟22万越南盾。一般换算是1元人民币3000越南盾 出国游最好手机上下一个google地图，高德在国外没有用的！ 👇多图预警，依然是想到什么再补充 DAY1越南时间凌晨飞机降落在胡志明市，办签证过海关入境，乘车去酒店，零零碎碎收拾一下最后快三点才睡下 七点起床收拾退房吃早餐，八点半出发四个小时车程到美奈o(╥﹏╥)o蓝天白云真好看 仙女溪→红沙丘，全程吃沙，不过踩着细沙还蛮舒服哒 晚餐自行解决，结果就是我们走了将近半小时找一个美团上的餐厅没找到，返回的路上看见了乐天，进去逛了一圈。最后在酒店旁边一个类似大排档的地方吃了，敲好吃哒，因为语言不通老板还给我们降价了*(:з」∠)*美奈人民真朴实 DAY2上午去的白石丘，沙地摩托真的刺激！将近90度的坡俯冲下去的一瞬间，叫都叫不出来只敢把眼睛闭着，抱紧越南小哥哥的腰*(:з」∠)* 刺激完后又是四个小时的车程去芽庄，占婆塔→五指岩 甘蔗汁很甜，五指岩对面有个川菜馆……晚上住的是超大海景房，落地玻璃窗，自带滤镜 DAY3今天是海岛一日游！水真的很清，玻璃窗以后不会再坐了，玻璃没擦干净，而且没几条鱼。体验了飞伞，超爽哒！ 上岸之后体验泥浆浴，感觉是兑了水的，泡泥浆浴最好穿次一些的泳衣，最好是那种准备扔了的泳衣。因为泡完之后泳衣上也会有很多泥浆，出门在外又不好洗，很烦 DAY4逛购物店的一天…… 本来说有个网红冰激凌，结果人家因为春节关门休息了，最后我吃的是和路雪巧克力味甜筒…… 不过晚饭恰的是瓦片烤肉，很棒！很满足！ PS：感觉越南人的心算有点堪忧啊，饮料需要自己点，结果服务员算了好久，好在可以用中文和她交流，不然得疯 DAY5又是早起的一天，四点不到起床收拾赶飞机，又到胡志明，参观了红教堂、百年邮局、歌剧院、市政厅，喝了咖啡，吃完饭之后就是自由行啦！ 美术馆是个拍照的好地方，可我真不喜欢拍照呀QAQ求放过 范五老街不是老街，这人就叫范五老*(:з」∠)*，在这条街上找到一个吃越南菜的地方，好吃到哭，人均换算成人民币50-60r的样子，也还好对吧 自由行是真滴快乐，嘿嘿嘿 "},{"title":"HEXO搭建踩过的坑（bu","date":"2019-01-27T12:42:00.000Z","url":"/2019/01/27/HEXO%E6%90%AD%E5%BB%BA%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91%EF%BC%88bu/","tags":[["hexo","/tags/hexo/"]],"categories":[["教程文档","/categories/%E6%95%99%E7%A8%8B%E6%96%87%E6%A1%A3/"]],"content":"很早就想写了，为什么拖到现在呢？拖延症不行啊！╭(╯^╰)╮ 其实也不是踩过的坑，就想随便写些什么相关的，避免以后再犯（？？？ 最初是18年的3,4月份，被室友推荐了GitHub的博客，结果Git一直下不下来，后来选主题也选了很久，想要一个又简洁又好看的，带点啥装饰物最好了（？？？ Hexo的主题实在是太多啦，最后还是选定了fexo_(:з」∠)_，但是现在换成了Kratos-Rebirth，萌萌二次元！ 真正开始搭博客是今年一月份了，虽然也没写啥。毕竟没有学习，也没啥可写的。隔得时间太久所以一开始有些东西就忘记放哪了，找位置花了点时间*(:з」∠)* 透明ico是可以做的！拿一张背景透明的png找在线网站转就好了。一定要png，因为jpg导出的时候背景会自动变成白色的 有些东西不能随便删，因为不知道写这个主题的人在哪里用了这些东西emmmm，删了可能会出错，true改成false就好啦。PS：Git Bash的报错还是得认真看看的ovo 最上面的title，date，categories等等，冒号后面是要有个空格的，不然会凉 markdown是不会给你多空行的！想要两个段落之间有一定的距离，需要加上&lt;/br&gt; 👆如果想到什么再补充叭 👇记一下git的命令 新建一个网站。如果没有设置folder，Hexo默认在目前的文件夹建立网站。 新建一篇文章。如果没有设置layout的话，默认使用_config.yml中的default_layout参数代替。如果标题包含空格的话，请使用引号括起来。 生成静态文件 文件生成后立即部署网站 监视文件变动 发表草稿 启动服务器。默认情况下，访问网址为： 重设端口 只使用静态文件 启动日记记录，使用覆盖记录格式 $ hexo render &lt;file1&gt; [file2] ...渲染文件 设置输出路径$ hexo migrate &lt;type&gt;从其他博客系统迁移内容 $ hexo clean清除缓存文件 (db.json) 和已生成的静态文件 (public)。 在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 $ hexo list &lt;type&gt;列出网站资料 $ hexo version显示 Hexo 版本 $ hexo --safe在安全模式下，不会载入插件和脚本。当您在安装新插件遭遇问题时，可以尝试以安全模式重新执行。 $ hexo --debug在终端中显示调试信息并记录到debug.log。当您碰到问题时，可以尝试用调试模式重新执行一次，并提交调试信息到 GitHub。 $ hexo --silent隐藏终端信息。 $ hexo --config custom.yml自定义配置文件的路径，执行后将不再使用_config.yml。 $ hexo --draft显示source/_drafts文件夹中的草稿文章。$ hexo --cwd /path/to/cwd自定义当前工作目录（Current working directory）的路径。"},{"title":"从零到0的编辑器学习","date":"2019-01-18T12:36:53.000Z","url":"/2019/01/18/%E4%BB%8E%E9%9B%B6%E5%88%B00%E7%9A%84%E7%BC%96%E8%BE%91%E5%99%A8%E5%AD%A6%E4%B9%A0/","tags":[["剑三","/tags/%E5%89%91%E4%B8%89/"],["编辑器","/tags/%E7%BC%96%E8%BE%91%E5%99%A8/"]],"categories":[["剑三","/categories/%E5%89%91%E4%B8%89/"]],"content":"2018.5.8艰难的打开了编辑器，加载了扬州地图，然后！怎么是一片蓝的啊！怎么滑滚轮都没有用啊！我的扬州呢！十几分钟后，哦，滚轮滑的不够多*(:з」∠)*艰难的调到了想要的画面，室友：“哇，这个直接截图就可以交场景作业了啊！”。我：“卧槽！对哦！我场景是照着这个做的，做的稀巴烂。我当时为什么没想到直接截图。”这大概就是傻吧…… 认真的在b站找到了一个重置版编辑器的教程，还在记笔记……然后，就去看动漫了emmmm角色什么的，明天再弄嘿嘿 5.9-5.10（被催更了扣诶扣）（题外话：从选择无垢这首歌作为作业的时候，脑子里一直单曲循环这首歌，好不容易今天没有想无垢了，又开始单曲循环童话镇，谁来救救我OTZ）拖延症晚期所以进展缓慢，两天时间过去了才把主要角色捏好。网上找的捏脸好多都带贴花，或者眼睛颜色是黄的啊什么的*(:з」∠)*然后就打开剑三客户端一个个的改掉。发现有些头发会有bug，服装的可选择范围又缩小了。成男的捏脸真是太难找啦！看上去都差不多，最后只能 “算了，管他和门派搭不搭，帅就行了”。啊，突然发现女主长大后的脸还要重新调过（哇，要不是这篇日志我还没发现扣诶扣）配图是放了一些角色进地图的样子（只是单纯的选了个位置放了进去，所以还是大张着手的样子，有个路人NPC噢）。 5.26-5.28（很久很久没更新了是不是）做好了男主女主扬州分开之后的场景，导出的avi文件真的好大啊，一个三四秒的视频可以有500m。做的时候就觉得湖面有点怪怪的，想着渲染之后应该会好的吧，并没有。更新了最新版本的编辑器，看来是要把做的所有重新导出一下了，还好为了省事每个镜头都重新拖了角色进去。看了看大大做的视频，开始思考要不要适当加一些镜头的运动。是不是要k帧呀，是不是还要考虑镜头移动的速度啊，好麻烦呀。陷入沉思这是一个可爱的男孩子（和他的隼）以及一个可爱的女孩子（和她的扇子舞） 6.9-6.10骨骼动画get，滥竽充数能力get恭喜男配角军爷杀青，✿✿ヽ(°▽°)ノ✿恭喜男嘉宾牵手女嘉宾，成功私（不）奔（是），✿✿ヽ(°▽°)ノ✿不想做了，这玩意伤身体啊……心态崩了，没有配图 不知道几号开始-6.29总算是把视频做完交掉了，剪辑了部分别人的视频拿过来用，自己做的镜头总共有26.5G，真可怕，最终从ae里导出来的avi文件有40G。无损压缩视频弄了我一下午，一方面是总是不满意压缩出来的质量，觉得太模糊了，另一方面是源文件太大了，压缩起来自然很慢。记一些东西好了，说不定以后会用到呢：P001,P008,P012男性NPC角色，P012是大块头的那种，肌肉男P004,P005,P006女性NPC角色NPC好像都没有上半身动画，只有角色动画，每次选动作的时候可以先搜索pst和st，走路是wlk，跑是run小女孩，标准女，小男孩，标准男都是玩家角色玩家角色里面1000-2000都是NPC的服饰，2000开始是门派校服，到2910截止，除了苍云长歌霸刀外所有的门派校服，这些门派的校服在最下面，其余需要的服饰可以在excel找，需要的颜色在偏色里调。头可以选1000或1001，这样可以避免穿模和叠两个头发上去。头里面的基本不会用到，需要头发的时候在帽子里找。门派校服可以先把手，腰带，腿等选为NULL，选好衣服之后点击加载同名插件。如果穿商城衣服必须把手，腰带，腿等选为NULL，不然会穿模。游戏里的表情里面有个作揖，在编辑器里好像是拱手，还有一个是书生行礼（这个有鞠躬的！）。如果是搜门派动作可以搜门派拼音首字母骨骼动画每动一下都要k帧上下半身动画指上半身的动画，角色动画指下半身的动画（没有上下半身动画的时候就是全身动画），两个是可以叠加的emmmm其他的以后如果有遇到再记吧，放一张视频的封面图好了，用的是芙蕖大大的数据（我才没有能力自己捏呢哼）"},{"title":"Markdown语法与Typora快捷键","date":"2019-01-16T11:45:04.000Z","url":"/2019/01/16/Markdown%E8%AF%AD%E6%B3%95%E4%B8%8ETypora%E5%BF%AB%E6%8D%B7%E9%94%AE/","tags":[["Markdown","/tags/Markdown/"],["Typora","/tags/Typora/"]],"categories":[["教程文档","/categories/%E6%95%99%E7%A8%8B%E6%96%87%E6%A1%A3/"]],"content":"标题 Markdown语法 Typora快捷键 # 一级标题 Ctrl+1：一级标题 ## 二级标题 Ctrl+2：二级标题 ### 三级标题 Ctrl+3：三级标题 #### 四级标题 Ctrl+4：四级标题 ##### 五级标题 Ctrl+5：五级标题 ###### 六级标题 Ctrl+6：六级标题 Ctrl+0：段落 粗体、斜体、删除线和下划线 Markdown语法 Typora快捷键 *斜体* Ctrl+I：斜体 **粗体** Ctrl+B：粗体 ***加粗斜体*** Ctrl+U：下划线 ~~删除线~~ Alt+Shift+5：删除线 超链接Markdown语法： Typora快捷键： Ctrl+K 引用Markdown语法： Typora快捷键： Ctrl+Shift+Q 代码Markdown语法： 为方便起见，这是使用字符a代替反引号` Typora快捷键： Ctrl+Shift+` 分割线Markdown语法： 以上两种方法均可以表示为分割线 列表Markdown语法： 以上三种方法均可以表示为列表 表格Markdown语法： Typora快捷键： Ctrl+T 图片Markdown语法： Typora快捷键： Ctrl+Shift+I 说明：在Hexo中插入图片时，请按照以下步骤进行设置 -将站点配置文件中的post_asset_folder选项设置成true -在站点文件夹中打开git bash，输入命令npm install hexo-asset-image --save安装插件 -此时使用hexo new title创建文章时，将同时在source/_post文件夹中生成一个与title同名的文件夹，我们只需将待添加的图片放进此文件夹中，然后在文章中通过Markdown语法进行引用即可。例如，在资源文件夹（就是那个与title同名的文件夹）中添加了图片example.PNG，则可以在对应的文章中使用语句![示例图片](title/example.PNG &quot;示例图片&quot;)添加图片"}]